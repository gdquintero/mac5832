{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAC0460 / MAC5832 (2020)\n",
    "<hr>\n",
    "\n",
    "## EP3: Linear regression + gradient descent\n",
    "\n",
    "### Topics explored:\n",
    "\n",
    "- implementation and test of the *batch gradient descent* algorithm for solving the linear regression problem \n",
    "- understanding cost functions and the core idea of gradient descent\n",
    "- comparison of both solutions (the analytic and the gradient descent based ones)\n",
    "\n",
    "### The evaluation will consider \n",
    "- correctitude of the algorithms\n",
    "- appropriateness of the answers\n",
    "- Code\n",
    "    - do not change the prototype of the function\n",
    "    - efficiency (you should avoid unnecessary loops; use matrix/vector computation with NumPy wherever appropriate)\n",
    "    - cleanliness (do not leave any commented code or useless variables)\n",
    "- File format: Complete and submit this notebook. **PLEASE do no change the file name.**\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "See EP2 to recall some terms and concepts.\n",
    "See also the reference materials pointed in class log notes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "Gradient descent is a technique used to find the minimum of a function. Here we will use it to compute the minimum of the cost function $J:\\mathbb{R}^{1+d} \\rightarrow \\mathbb{R}$:\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\mathbf{w}) = \\frac{1}{N}\\sum_{i=1}^{N}\\big(\\hat{y}^{(i)} - y^{(i)}\\big)^{2}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\hat{y}^{(i)} = \\mathbf{w}^T\\mathbf{\\tilde{x}}^{(i)}$, $\\mathbf{\\tilde{x}} = (1, x_1, \\ldots, x_d)$.\n",
    "\n",
    "We start at a random point $\\mathbf{w}(0) \\in \\mathbb{R}^{1+d}$ at $t=0$, and at each iteration $t$ we compute the gradient of $J$ at $\\mathbf{w}(t)$, and then update $\\mathbf{w}(t)$ by a vector proportional to  $-\\nabla_{\\mathbf{w}}J(\\mathbf{w})$ (the negative of the gradient).\n",
    "\n",
    "- $\\mathbf{w}(0) = \\mathbf{w}$\n",
    "- for $t = 0, 1, 2, \\dots$ do\n",
    "    * $\\mathbf{w}(t+1) = \\mathbf{w}(t) - \\eta \\nabla_{\\mathbf{w}(t)}J(\\mathbf{w}(t))$ \n",
    "\n",
    "The parameter $\\eta \\in \\mathbb{R}_{\\geq}$, known as **learning rate** controls the size of the updates. This parameter could be adaptivelly changed along the iterations, but we will keep it fixed (we will use the value defined at the beginning of the process).\n",
    "\n",
    "### Gradient\n",
    "\n",
    "The partial derivative of $J(\\mathbf{w})$ with respect to each component $j$, $j=0,1,\\ldots,d$, of $\\mathbf{w}$ is given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\mathbf{w})}{\\partial w_{j}} = \\frac{2}{N}\\sum_{i=1}^{N} (\\hat{y}^{(i)} - y^{(i)})\\, x^{(i)}_j\n",
    "\\end{equation}\n",
    "\n",
    "Thus the gradient of $J(\\mathbf{w})$ with respect to $\\mathbf{w}$ is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{\\mathbf{w}}J(\\mathbf{w}) = \\begin{bmatrix}\\frac{\\partial J(\\mathbf{w})}{\\partial w_{0}} \\dots \\frac{\\partial J(\\mathbf{w})}{\\partial w_{d}} \\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and auxiliary functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from util.util import get_housing_prices_data, r_squared\n",
    "\n",
    "    \n",
    "def simple_step_plot(ylist, yname, title, figsize=(4, 4), labels=None):\n",
    "    y0 = ylist[0]\n",
    "    x = np.arange(1, len(y0) + 1, 1)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    for y in ylist:\n",
    "        ax.plot(x, y)\n",
    "    plt.xlabel('step')\n",
    "    plt.ylabel(yname)\n",
    "    plt.title(title,\n",
    "              fontsize=14,\n",
    "              fontweight='bold')\n",
    "    plt.grid(True)\n",
    "    if labels is not None:\n",
    "        plt.legend(labels,\n",
    "           loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset \n",
    "\n",
    "Here we will use the *Boston Housing Prices*, available for download in `sklearn`, as shown below. See a description of the dataset for instance at https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html .\n",
    "Due to visualization constraints, we use only one input variable (LSTAT) to predict the price of a house (in $1000s).\n",
    "\n",
    "(you can also try using other variables or set of variables, or experiment with the same dataset of EP2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = load_boston()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "X = df.LSTAT.values.reshape((-1,1))\n",
    "y = data.target.reshape((-1,1))\n",
    "print(\"X shape: \", X.shape)\n",
    "print(\"y shape: \", y.shape)\n",
    "\n",
    "plt.scatter(X[:,0], y, alpha=0.4)\n",
    "plt.xlabel('LSTAT')\n",
    "plt.ylabel('median value in $1000')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Write the code for the functions specified below, using matrix/vector computation with NumPy. Note that each function requires only a few lines of code:\n",
    "\n",
    "- computation of the cost function value\n",
    "- computation of the gradient\n",
    "- optimization of the cost function: application of the gradient descent technique\n",
    "- computation of the prediction\n",
    "\n",
    "Note also that the dataset is input to some of the functions in its original shape $N\\times d$ while it is input in the extended shape $N \\times (1+d)$ for the others "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computation of the cost function value\n",
    "def compute_cost(Xe, y, w):\n",
    "    \"\"\"\n",
    "    Calculates  mean square error cost.\n",
    "\n",
    "    :param Xe: design matrix\n",
    "    :type Xe: np.ndarray(shape=(N, 1+d))\n",
    "    :param y: regression targets\n",
    "    :type y: np.ndarray(shape=(N, 1))\n",
    "    :param w: weights\n",
    "    :type w: np.array(shape=(1+d, 1))\n",
    "    :return: cost\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "\n",
    "    # START OF YOUR CODE:\n",
    "    raise NotImplementedError(\"Function compute_cost() is not implemented\")\n",
    "    # END YOUR CODE\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computation of the gradient\n",
    "def compute_wgrad(Xe, y, w):\n",
    "    \"\"\"\n",
    "    Calculates gradient of J(w) with respect to w.\n",
    "\n",
    "    :param Xe: design matrix\n",
    "    :type Xe: np.ndarray(shape=(N, 1+d))\n",
    "    :param y: regression targets\n",
    "    :type y: np.ndarray(shape=(N, 1))\n",
    "    :param w: weights\n",
    "    :type w: np.array(shape=(1+d, 1))\n",
    "    :return: gradient\n",
    "    :rtype: np.array(shape=(1+d, 1))\n",
    "    \"\"\"\n",
    "\n",
    "    # START OF YOUR CODE:\n",
    "    raise NotImplementedError(\"Function compute_wgrad() is not implemented\")\n",
    "    # END YOUR CODE\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **batch gradient descent** algorithm\n",
    "\n",
    "At each iteration (epoch), the whole dataset is used to update the weight vector. \n",
    "The function should iterate <tt>num_epochs</tt>\n",
    "times and return the final weight vector as well as the *cost history*, a list containing the cost (E_in) computed before starting the iterations plus at the end of each iteration.\n",
    " \n",
    "A left column of 1's should be add to the data matrix $X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization of the cost function:\n",
    "# application of the gradient descent technique\n",
    "\n",
    "def batch_gradient_descent(X, y, w, learning_rate=0.001, num_epochs=100):\n",
    "    \"\"\"\n",
    "     Performs batch gradient descent optimization.\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param y: regression targets\n",
    "    :type y: np.ndarray(shape=(N, 1))\n",
    "    :param w: weights\n",
    "    :type w: np.array(shape=(1+d, 1))\n",
    "    :param learning_rate: learning rate\n",
    "    :type learning_rate: float\n",
    "    :param num_epochs: number of iterations\n",
    "    :type num_iters: int\n",
    "    :return: weights, cost history\n",
    "    :rtype: np.array(shape=(1+d, 1)), list\n",
    "    \"\"\"\n",
    "    \n",
    "    # START OF YOUR CODE:\n",
    "    raise NotImplementedError(\"Function batch_gradient_descent() is not implemented\")\n",
    "    # END YOUR CODE\n",
    "\n",
    "    return w_current, cost_history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prediction(X, w):\n",
    "    \"\"\"\n",
    "    Calculates the prediction over a set of observations X using\n",
    "    the linear function characterized by the weight vector w.\n",
    "    \n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param w: weight vector\n",
    "    :type w: np.ndarray(shape=(1+d, 1))\n",
    "    :param y: regression prediction\n",
    "    :type y: np.ndarray(shape=(N, 1))\n",
    "    \"\"\"\n",
    "    \n",
    "    # START OF YOUR CODE:\n",
    "    raise NotImplementedError(\"Function compute_prediction() is not implemented\")\n",
    "    # END YOUR CODE\n",
    "        \n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Training\n",
    "\n",
    "The code below uses the functions above.\n",
    "\n",
    "Experiment varying the *learning rate*, the *initial weight vector*, and the *number of iterations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "initial_w = np.array([[20], [0]])\n",
    "print(\"Initial weight:\\n\", initial_w)\n",
    "iterations = 500\n",
    "\n",
    "w, cost_history = batch_gradient_descent(X, y, initial_w, \\\n",
    "                                         learning_rate, iterations)\n",
    "\n",
    "prediction = compute_prediction(X,w)\n",
    "\n",
    "print(\"Final weight:\\n\", w)\n",
    "\n",
    "r_2 = r_squared(y, prediction)\n",
    "print(\"R squared = \", r_2)\n",
    "\n",
    "simple_step_plot(\n",
    "    [cost_history], 'loss',\n",
    "    'Training loss\\nlearning rate = {} | iterations = {}'.format(learning_rate, iterations))\n",
    "\n",
    "plt.scatter(X[:,0], y, alpha=0.4)\n",
    "x_reg = np.linspace(0, 40, 100)\n",
    "y_reg = initial_w[1]*x_reg + initial_w[0]\n",
    "plt.plot(x_reg, y_reg, c='g')\n",
    "y_reg = w[1]*x_reg + w[0]\n",
    "plt.plot(x_reg, y_reg, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3:\n",
    "*Concisely* write here the experiments you have performed and any interesting comments you have based on the experiments (max 10 lines):\n",
    "\n",
    "-\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Comparison\n",
    "\n",
    "- Use the normal equation code (analytical solution) of your EP2 to process the same dataset above.\n",
    "- Print the final weight vector\n",
    "- Compute the prediction using function <tt>compute_prediction()</tt> above\n",
    "- Compute and print the R-squared metric\n",
    "- Use the normal equation code (analytical solution) of your EP2 to process the same dataset above.\n",
    "- Plot a graph with the examples and the function $\\mathbf{w}^T\\mathbf{x}$\n",
    "- Add a text cell, and write any comments regarding what you have observed/learned by comparing the *batch gradient descent* and the analytical solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_equation_weights(X, y):\n",
    "    \"\"\"\n",
    "    Calculates the weights of a linear function using the normal equation method.\n",
    "    You should add into X a new column with 1s.\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param y: regression targets\n",
    "    :type y: np.ndarray(shape=(N, 1))\n",
    "    :return: weight vector\n",
    "    :rtype: np.ndarray(shape=(1+d, 1))\n",
    "    \"\"\"\n",
    "    \n",
    "    # START OF YOUR CODE:\n",
    "    raise NotImplementedError(\"Function normal_equation_weights() is not implemented\")\n",
    "    # END YOUR CODE\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to run and show results of the analytical solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your comments (max 10 lines) here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extras (no bonus, just for fun)\n",
    "\n",
    "1. If you are curious, you may also implement the **stochastic gradient descent** algorithm and compare it with the batch version. Recal that in the stochastic version, only one example is used to compute the cost and update the weight vector. There will be two layers of iteration: one for iterating over the examples in the training set (example by example) -- one of these iterations correspond to an epoch; and a second one to iterate multiple times over the set (that is, the number os times you scan the whole set corresponds to the number of epochs)\n",
    "2. You may also try different variables of the Boston Housing Prices dataset in $X$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
